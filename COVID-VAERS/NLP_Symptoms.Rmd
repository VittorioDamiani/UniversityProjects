---
title: "Symptoms"
author: "BABDGroup3"
---

```{r}
library('tm')
library('proxy')
library('ggplot2')
library('wordcloud')
library('textstem')
```

Import data
```{r}
serious <- read.csv(file="2021VAERSDATA+VAX_serious.csv") 
non_serious <- read.csv(file="2021VAERSDATA+VAX_nonserious.csv")
```

**************************
* Serious - SYMPTOM_TEXT *
**************************

Remove rows with missing SYMPTOM_TEXT
```{r}
serious1 <- subset(serious, SYMPTOM_TEXT=!"")
```

Load SYMPTOM_TEXT as corpus
```{r}
symptom <- VCorpus(VectorSource(serious1$SYMPTOM_TEXT)) 
symptom[[5]]$content
```

----------------------------Text cleaning-----------------------------------------------------------------------------

1) Remove punctuation and numbers
```{r}
symptom <- tm_map(symptom,removePunctuation)
symptom <- tm_map(symptom,removeNumbers)
symptom[[5]]$content
```

2) Convert to lowercase
```{r}
symptom <- tm_map(symptom,content_transformer(tolower))
symptom[[5]]$content
```

3) Remove stopwords and strip unnecesary whitespace
```{r}
symptom <- tm_map(symptom, removeWords, stopwords("english"))
symptom <- tm_map(symptom, stripWhitespace)
symptom[[5]]$content
```

Lemmatize using textstem
```{r}
library(textreg)
strings <- convert.tm.to.character(symptom)
```

```{r}
strings_lem <- lemmatize_strings(strings)
```

```{r}
symptom_lem <- VCorpus(VectorSource(strings_lem))
symptom_lem[[5]]$content
```

-------------------------------Document-terms matrix------------------------------------------------------------------

Create the document-terms matrix
```{r}
dtm <- DocumentTermMatrix(symptom_lem)
dtmmtx <- as.matrix(dtm)
```

Terms and their frequencies
```{r}
freq <- colSums(as.matrix(dtm)) 
wf <- data.frame(word=names(freq),freq=freq)
```

```{r}
dtms <- removeSparseTerms(dtm, 0.93)
freq <- colSums(as.matrix(dtms)) 
wf <- data.frame(word=names(freq),freq=freq)
```

Manually remove some "common" words
```{r}
'%!in%' <- function(x,y)!('%in%'(x,y))
wf <- subset(wf, wf$word %!in% c("patient", "vaccine", "covid", "report", "receive", "day", "dose", "event", "hospital",
                                 "vaccination", "take", "minute", "hour", "low", "appropriate", "room", "get", "feel",
                                 "good", "take", "pfizerbiontech", "unknown", "pfizer", "female", "notify", "product",
                                 "week", "appropriate", "continue", "begin", "start", "blood", "information", "result",
                                 "promptly", "single", "evaluation", "morning"))
```

------------------------------Data viz--------------------------------------------------------------------------------

Display the remaining terms according to their frequencies
```{r}
p <- ggplot(subset(wf),aes(word, freq))    
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=90, hjust=1))   
p
```

Display the clouds of words
```{r fig.height=7, fig.width=7}
png('Symptoms_Serious.png')
wordcloud(wf$word,wf$freq,scale=c(5,0.1),colors=brewer.pal(6,"Dark2")) 
```

***************
* Non-Serious *
***************

Remove rows with missing SYMPTOM_TEXT
```{r}
non_serious <- subset(non_serious, SYMPTOM_TEXT=!"")
```

Load SYMPTOM_TEXT as corpus
```{r}
symptom2 <- VCorpus(VectorSource(non_serious$SYMPTOM_TEXT)) 
symptom2[[5]]$content
```

----------------------------Text cleaning-----------------------------------------------------------------------------

1) Remove punctuation and numbers
```{r}
symptom2 <- tm_map(symptom2,removePunctuation)
symptom2 <- tm_map(symptom2,removeNumbers)
symptom2[[5]]$content
```

2) Convert to lowercase
```{r}
symptom2 <- tm_map(symptom2,content_transformer(tolower))
symptom2[[5]]$content
```

3) Remove stopwords and strip unnecesary whitespace
```{r}
symptom2 <- tm_map(symptom2, removeWords, stopwords("english"))
symptom2 <- tm_map(symptom2, stripWhitespace)
symptom2[[5]]$content
```

Lemmatize using textstem
```{r}
library(textreg)
strings2 <- convert.tm.to.character(symptom2)
```

```{r}
strings_lem2 <- lemmatize_strings(strings2)
```

```{r}
symptom_lem2 <- VCorpus(VectorSource(strings_lem2))
symptom_lem2[[5]]$content
```

-------------------------------Document-terms matrix------------------------------------------------------------------

Create the document-terms matrix
```{r}
dtm2 <- DocumentTermMatrix(symptom_lem2)
dtmmtx2 <- as.matrix(dtm2)
```

Terms and their frequencies
```{r}
freq2 <- colSums(as.matrix(dtm2)) 
wf2 <- data.frame(word=names(freq2),freq=freq2)
```

```{r}
dtms2 <- removeSparseTerms(dtm2, 0.99)
freq2 <- colSums(as.matrix(dtms2)) 
wf2 <- data.frame(word=names(freq2),freq=freq2)
```

Manually remove some "common" words
```{r}
'%!in%' <- function(x,y)!('%in%'(x,y))
wf2 <- subset(wf2, wf2$word %!in% c("covid", "patient", "jan", "event", "day", "feel", "include", "start", "unknown",
                                   "unspecified",  "single", "pfizerbiontech", "report", "outcome", "minute",
                                   "week", "vaccine", "dose", "arm", "injection", "receive", "leave"))
```

------------------------------Data viz--------------------------------------------------------------------------------

Display the remaining terms according to their frequencies
```{r}
p2 <- ggplot(subset(wf2),aes(word, freq))    
p2 <- p2 + geom_bar(stat="identity")   
p2 <- p2 + theme(axis.text.x=element_text(angle=90, hjust=1))   
p2
```

Display the clouds of words
```{r fig.height=7, fig.width=7}
png('Symptoms_Non_Serious.png')
wordcloud(wf2$word,wf2$freq,scale=c(5,0.1),colors=brewer.pal(6,"Dark2")) 
```




























































